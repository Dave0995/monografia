{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b2f102",
   "metadata": {},
   "source": [
    "# Modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c298704",
   "metadata": {},
   "source": [
    "En esta secci贸n se cargaran todos los datos que han sido transformados y limpiados para realizar feature engineering, optimizaci贸n de hiperparametros y reducci贸n de dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39970ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6374fa9",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59419580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datablob.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85dd85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391526, 82)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c9ca6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecci贸n de modelos\n",
    "\n",
    "Para este analisis se realizaran las comparaciones de performance utilizando 3 modelos:\n",
    "\n",
    "1. Gradient Boosting tree\n",
    "2. Random Forest\n",
    "3. Support vector machine\n",
    "4. Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc71ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"gasto_familiar\"], axis = 1)\n",
    "Y = df[\"gasto_familiar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f765f05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelo sencillo para analisis de features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564d2bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "ENReg = ElasticNetCV(random_state = 0)\n",
    "ENReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b96990c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error2(y_pred, y_true):\n",
    "    y_true = np.where(y_true == 0, 0.0000000001, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de9900da-d882-4ce1-be8e-de0001a7370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ENReg.predict(X_test)\n",
    "y_train_pred = ENReg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd63050b-450e-41a6-b87d-c0fd1b4bce06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07699740947462008"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENReg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13ba215-3fd4-4831-bafd-0c6f9c30797a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07588585950748794"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENReg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bce383-0e0c-471c-8327-927f9f58e9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.45194498324102"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error2(y_pred, y_test)\n",
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65ee88-3d8a-4fce-92dd-a1afa3af8227",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df89d50-6d66-4f7b-bac1-61351f32f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.sample(frac=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd78bda2-eceb-4486-ba76-cc251eca0de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84578, 75)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc94d41-f216-4106-bc66-9eabe2e2051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = dft.drop([\"gasto_familiar\"], axis = 1)\n",
    "yt = dft[\"gasto_familiar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a65a078-539f-4d56-a2d5-47247314be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a9274b6-1c22-4ecf-99f9-95d70ac297b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENRegt = ElasticNetCV(random_state = 0)\n",
    "ENRegt.fit(Xt_train, yt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570ff6ee-5f7c-4a71-a321-5ede1c931ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05462604868592369"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENRegt.score(Xt_train, yt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "913d9b24-cea0-4b42-89b4-d21769385915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04640330702250495"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENRegt.score(Xt_test, yt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c240a96c-26bc-4636-8f67-d8963ffd5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26594091-c5ee-4f65-bc60-bb338eefe832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('sgdregressor', SGDRegressor())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = make_pipeline(StandardScaler(),\n",
    "                    SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74172581-6873-431b-aee2-0bc7c47f0651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1713768072.4578354"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dca7e2-f76c-4898-8a47-6bd06ce8b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed40d5d0-26d1-470b-b32e-6d46a701672f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=10)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "588899c8-cd47-4515-be2b-9ca5ee47cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "y_pre = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e3a26a6-0f55-43da-8c17-2122cedd4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.23873548941559"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error2(y_pre, y_test)\n",
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ecb1ea8-10d7-4a5a-9d1e-92e05e9c95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.sample(frac = 0.05)\n",
    "Xn = new_df.drop([\"gasto_familiar\"], axis = 1)\n",
    "yn = new_df[\"gasto_familiar\"]\n",
    "\n",
    "Xn_train, Xn_test, yn_train, yn_test = train_test_split(Xn, yn, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ab5b14-3efb-4dc1-b70b-2e71f2f1f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "ENRegr = ElasticNetCV(random_state = 12)\n",
    "scores = cross_validate(ENRegr, Xn, yn,return_train_score = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f8945e-a050-40a1-a60b-1ed8b9bf476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.50767851, 0.5270021 , 0.52464652, 0.51926494, 0.4890933 ]),\n",
       " 'score_time': array([0.00557613, 0.00551629, 0.00563979, 0.00584531, 0.00527501]),\n",
       " 'test_score': array([-0.44488124,  0.03290953,  0.04232167,  0.04415034,  0.04205877]),\n",
       " 'train_score': array([0.05763469, 0.0414832 , 0.03717012, 0.03704063, 0.03970907])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d89af835-b651-422d-bc66-e09805dad5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.04260754132214981\n",
      "Test: -0.05668818671829219\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd229be0-66fa-4eec-9e70-6c67670acfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "knn_scores = cross_validate(KNeighborsRegressor(n_neighbors=2), Xn, yn,return_train_score = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8364daa7-e973-40f3-8173-1ad374797feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.5622645734741077\n",
      "Test: -0.31884324454531254\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(knn_scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(knn_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9232f40b-38ec-45f6-9cb2-3d2b684656ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.5622645734741077\n",
      "Test: -0.31884324454531254\n"
     ]
    }
   ],
   "source": [
    "knn2_scores = cross_validate(KNeighborsRegressor(n_neighbors=2), Xn, yn,return_train_score = True, cv=5)\n",
    "print(f\"Entrenamiento: {np.mean(knn2_scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(knn2_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "966032db-46b0-4a0b-b493-b6c439bda41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.1492600061578848\n",
      "Test: 0.06098105173951176\n"
     ]
    }
   ],
   "source": [
    "knn20_scores = cross_validate(KNeighborsRegressor(n_neighbors=20), Xn, yn,return_train_score = True, cv=5)\n",
    "print(f\"Entrenamiento: {np.mean(knn20_scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(knn20_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8e4ee05-43ed-4adb-a1fe-82a63561e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "params = {'n_estimators': 800, 'max_depth': 16,\n",
    "          'loss': 'huber','alpha':0.95}\n",
    "\n",
    "GBReg = GradientBoostingRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee521ca0-51ba-4a32-8582-4f7dc7f16c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.9997914594835071\n",
      "Test: -0.011250145260617895\n"
     ]
    }
   ],
   "source": [
    "gbt_scores = cross_validate(GBReg, Xn, yn,return_train_score = True, cv=5)\n",
    "print(f\"Entrenamiento: {np.mean(gbt_scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(gbt_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0c429fa-65c0-4c92-8f01-ce3596bb127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21446b6f-ad06-4c6b-97a3-6e77131c709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 9506\n",
      "obj = -6823207855.603618, rho = -428924.074863\n",
      "nSV = 16914, nBSV = 16914\n",
      "[LibSVM].........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 9547\n",
      "obj = -6854443426.901639, rho = -430720.438374\n",
      "nSV = 16914, nBSV = 16914\n",
      "[LibSVM].........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 9539\n",
      "obj = -6857333962.521228, rho = -429160.926645\n",
      "nSV = 16914, nBSV = 16914\n",
      "[LibSVM].........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 9578\n",
      "obj = -6845934783.080253, rho = -433726.887896\n",
      "nSV = 16914, nBSV = 16914\n",
      "[LibSVM].........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 9525\n",
      "obj = -6787644724.178686, rho = -428094.925806\n",
      "nSV = 16916, nBSV = 16916\n",
      "Entrenamiento: -0.10972346820284593\n",
      "Test: -0.1099292147149041\n"
     ]
    }
   ],
   "source": [
    "svm_scores = cross_validate(SVR(C=1.0, epsilon=0.2, verbose=True), Xn, yn,return_train_score = True, cv=5)\n",
    "print(f\"Entrenamiento: {np.mean(svm_scores['train_score'])}\")\n",
    "print(f\"Test: {np.mean(svm_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9705ecb1-7795-4177-8572-47ec6d6a9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = new_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675bead-c883-4658-bb84-aa659728420a",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c4c506-9ad4-43db-a31e-340424d5f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 293328834777.8557  6868297764.4179            2.09m\n",
      "         2 282251552499.8793  5596691742.6740            2.05m\n",
      "         3 272985398405.9755  4469513343.7328            2.01m\n",
      "         4 266890291158.3091  3487372101.7636            1.98m\n",
      "         5 261550704118.8408  2717967295.3863            1.93m\n",
      "         6 257113919193.4347  2203513059.5363            1.89m\n",
      "         7 250443577929.1233  1626320592.8499            1.85m\n",
      "         8 247589211523.8410  1313704592.7379            1.81m\n",
      "         9 243780491701.0064  1032608818.5385            1.77m\n",
      "        10 240293041235.9603   794652771.9281            1.72m\n",
      "        11 238789023675.6021   665334912.7422            1.68m\n",
      "        12 234054458652.4223   428513647.5253            1.64m\n",
      "        13 232545759780.4030   392383876.9779            1.60m\n",
      "        14 228897538868.1555   184477171.1641            1.56m\n",
      "        15 228485719517.0646   180352065.1932            1.52m\n",
      "        16 226312907367.3759    64971296.3837            1.47m\n",
      "        17 222566898100.1548   -80864527.0232            1.43m\n",
      "        18 220129774266.6416   -30699067.4675            1.39m\n",
      "        19 219730794290.2415   -26504381.3326            1.35m\n",
      "        20 217228511606.9101   -31448117.0344            1.31m\n",
      "        21 216682931147.2332   -92841075.7136            1.27m\n",
      "        22 215580838426.0231   -88796234.5006            1.22m\n",
      "        23 212197024352.6096  -123562598.5944            1.18m\n",
      "        24 211480020610.2064  -180206378.6266            1.14m\n",
      "        25 211801839403.8817  -151945479.8539            1.10m\n",
      "        26 210016238552.8531  -139545898.7332            1.05m\n",
      "        27 208076559828.2090  -158082733.1276            1.01m\n",
      "        28 208254270113.8438   -79890629.7280           58.12s\n",
      "        29 205316860986.7845  -123558044.6441           55.50s\n",
      "        30 206310832963.1176  -129135138.6261           52.88s\n",
      "        31 204664380163.3519  -114568620.9239           50.24s\n",
      "        32 204345065231.8499   -94345596.8227           47.67s\n",
      "        33 202802157595.8684   -93395220.7645           45.03s\n",
      "        34 201945073084.3999  -131974379.9076           42.42s\n",
      "        35 203153828036.0897  -127428105.1366           39.84s\n",
      "        36 201845279069.8014  -111448394.8906           37.19s\n",
      "        37 200935681232.5791   -94820674.0535           34.58s\n",
      "        38 200351472737.0050  -100321039.5613           31.95s\n",
      "        39 199943640611.5485  -142841453.2858           29.31s\n",
      "        40 197869556514.9556  -233389812.9319           26.63s\n",
      "        41 196890248640.4570  -105655357.9340           23.96s\n",
      "        42 197334805972.4277   -86700480.6245           21.30s\n",
      "        43 196378404512.9599   -61750100.3514           18.64s\n",
      "        44 195323033025.4159  -150243135.9679           15.97s\n",
      "        45 195390497104.6343   -80848004.1604           13.32s\n",
      "        46 194300476335.4195  -103424930.4283           10.65s\n",
      "        47 192808871352.9602  -109626882.1590            7.99s\n",
      "        48 193777960660.5766   -94701322.9937            5.32s\n",
      "        49 192186513136.9339  -110704135.8999            2.66s\n",
      "        50 191629901971.3315   -91850661.3258            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 294444311896.4337  6939707679.1937            2.07m\n",
      "         2 283501971902.0312  5600238865.8167            2.03m\n",
      "         3 272697016464.7031  4406743125.3911            2.00m\n",
      "         4 266608605584.0973  3581875180.4048            1.96m\n",
      "         5 262901913236.4018  2698713797.6807            1.91m\n",
      "         6 256403430941.9872  2228713824.8874            1.87m\n",
      "         7 250456248337.3235  1716450279.5630            1.82m\n",
      "         8 246635328595.0633  1385230040.4796            1.78m\n",
      "         9 244546018634.0053  1098949712.3544            1.74m\n",
      "        10 240549499644.1192   705117216.1910            1.70m\n",
      "        11 237758595171.3362   614622296.5792            1.65m\n",
      "        12 233701816444.2810   441740899.5597            1.61m\n",
      "        13 231022280138.7484   373328319.1832            1.57m\n",
      "        14 228653762808.1043   256579074.7902            1.53m\n",
      "        15 227605841598.6737   147580545.4981            1.49m\n",
      "        16 225878374696.6058    44631438.2593            1.45m\n",
      "        17 224058564736.3160     9885885.4433            1.41m\n",
      "        18 221836391300.3033    -1061313.1843            1.37m\n",
      "        19 220354684774.7640   -32175382.8627            1.33m\n",
      "        20 217585753051.1588   -99916484.8432            1.29m\n",
      "        21 217664251945.7922   -66062174.0023            1.25m\n",
      "        22 215903310185.9330  -170789267.3772            1.21m\n",
      "        23 213978964768.8767  -171244292.4169            1.16m\n",
      "        24 211229621771.3873  -209586035.0220            1.12m\n",
      "        25 212997588146.1358  -123834728.7143            1.08m\n",
      "        26 210180175806.3175   -77889031.0630            1.04m\n",
      "        27 208779500064.8437  -153232612.1735           59.68s\n",
      "        28 208446609625.3075  -175019681.1237           57.13s\n",
      "        29 205330700988.6145   -86091307.0304           54.60s\n",
      "        30 207350190963.6458   -98704045.6775           52.00s\n",
      "        31 204257614100.1859  -160040721.6539           49.43s\n",
      "        32 204343388903.2912   -82040707.1752           46.87s\n",
      "        33 202822904308.8700  -145162903.8181           44.28s\n",
      "        34 201816642501.6136   -75533535.8568           41.72s\n",
      "        35 202733169290.2716  -153393975.6681           39.17s\n",
      "        36 199198943598.3565  -188206393.7033           36.54s\n",
      "        37 199603138865.5178   -63697167.8414           33.96s\n",
      "        38 198945622146.1350  -116319010.7769           31.35s\n",
      "        39 199837152359.0236  -102528772.7273           28.76s\n",
      "        40 199740541942.3214  -103933568.4970           26.16s\n",
      "        41 197786668542.3620   -61889835.3937           23.55s\n",
      "        42 197444696420.7966   -84867452.6595           20.96s\n",
      "        43 195274913670.1811  -167208387.5318           18.34s\n",
      "        44 195984916831.2281   -73937679.0382           15.72s\n",
      "        45 195364259215.0422   -72703776.5952           13.11s\n",
      "        46 193275490538.9714  -134725294.6175           10.49s\n",
      "        47 194661845031.7537   -75364189.4287            7.86s\n",
      "        48 193082289174.8018  -160938691.3867            5.24s\n",
      "        49 192692632672.2472  -130116132.4656            2.62s\n",
      "        50 192368485072.4443   -60590135.4215            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 294464536154.8582  6980160905.9633            2.08m\n",
      "         2 282044485665.4380  5448760149.7598            2.03m\n",
      "         3 272972464148.8442  4354873647.1646            2.00m\n",
      "         4 267533398816.8171  3443093292.5025            1.97m\n",
      "         5 261806273727.9099  2829072693.9544            1.92m\n",
      "         6 255954308627.7593  2218232383.4889            1.88m\n",
      "         7 251776636654.3162  1886173078.8391            1.84m\n",
      "         8 247113072618.9442  1355689367.9578            1.80m\n",
      "         9 245434263801.4573  1062623112.5441            1.76m\n",
      "        10 240397811384.2138   738530591.8497            1.72m\n",
      "        11 238737930500.6208   548841569.9097            1.68m\n",
      "        12 233834889933.3976   456228939.2904            1.64m\n",
      "        13 231261533858.7744   302678299.0480            1.60m\n",
      "        14 230330477971.0142   206632840.2026            1.55m\n",
      "        15 228229364432.3605   124125782.5621            1.51m\n",
      "        16 225637925075.2156    50930278.9143            1.47m\n",
      "        17 224754261368.5739    65816476.1493            1.43m\n",
      "        18 221575712113.5230    -4414809.3357            1.39m\n",
      "        19 219976009085.9357   -54665155.1404            1.35m\n",
      "        20 218866525396.9316  -100002636.6708            1.31m\n",
      "        21 218212296376.9165   -62835899.3350            1.26m\n",
      "        22 216882752470.6010  -127219170.3950            1.22m\n",
      "        23 213281061732.3946  -162215217.1335            1.18m\n",
      "        24 212352894420.6817  -169407980.9979            1.14m\n",
      "        25 211878210415.1266  -153614918.7209            1.10m\n",
      "        26 210506500491.1638  -113196219.3751            1.05m\n",
      "        27 209000655231.1433  -100963701.9763            1.01m\n",
      "        28 209226954990.3818  -129374723.1768           58.13s\n",
      "        29 206234739045.1099   -97216647.7274           55.56s\n",
      "        30 207681612354.7837   -89641433.6310           52.96s\n",
      "        31 205169022556.0153  -161400285.2149           50.31s\n",
      "        32 202880224506.1103  -180449437.3004           47.65s\n",
      "        33 203239397812.5652  -119482586.1516           45.00s\n",
      "        34 202275272031.7698   -86770521.9710           42.42s\n",
      "        35 200650655097.2983  -215889006.0971           39.80s\n",
      "        36 200261324347.1180  -102109788.1339           37.15s\n",
      "        37 199461390919.3315   -60098326.6875           34.52s\n",
      "        38 199645727061.2871   -96056747.9160           31.85s\n",
      "        39 199247656850.5834  -148059274.5127           29.19s\n",
      "        40 198616287195.4014  -149258527.4265           26.56s\n",
      "        41 198167153823.6283   -82704618.9459           23.91s\n",
      "        42 196711435451.9860  -113379699.3075           21.27s\n",
      "        43 195069342148.4355  -198993104.9672           18.61s\n",
      "        44 193318475354.2018  -240260372.5256           15.94s\n",
      "        45 193626480669.9963  -122689405.4214           13.28s\n",
      "        46 192879618248.5894   -30311402.6636           10.64s\n",
      "        47 193667341100.6326   -79562121.7655            7.98s\n",
      "        48 191331839335.1210  -184921394.0080            5.32s\n",
      "        49 191585736381.3359   -67943601.5300            2.66s\n",
      "        50 191762374857.1431   -71847737.1821            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 293083173686.7731  6862054565.2655            2.08m\n",
      "         2 281246589956.2083  5499640494.2556            2.02m\n",
      "         3 272943538191.9362  4422257089.1102            1.97m\n",
      "         4 267159934660.9238  3585347816.8026            1.94m\n",
      "         5 262424757218.9370  2838361969.9956            1.91m\n",
      "         6 255683734261.4362  2261892913.9675            1.86m\n",
      "         7 250771255380.5904  1774590320.1316            1.82m\n",
      "         8 246533835604.0155  1405120420.6857            1.78m\n",
      "         9 244799563036.7750  1096830515.6915            1.74m\n",
      "        10 238863020179.1101   765282391.4133            1.70m\n",
      "        11 238047280442.6553   540974049.3720            1.66m\n",
      "        12 232818973857.9498   529016506.5146            1.62m\n",
      "        13 232649086647.4088   323489704.2069            1.58m\n",
      "        14 229375456635.8850   143020305.3643            1.54m\n",
      "        15 227124584365.4470    54052553.6515            1.50m\n",
      "        16 225101532205.9782   121746483.3338            1.46m\n",
      "        17 224859169851.1905   -25640296.3132            1.42m\n",
      "        18 221652548780.2383   -43292848.3352            1.38m\n",
      "        19 218949715737.5374    14767429.2681            1.34m\n",
      "        20 218726303452.2562   -80386733.9995            1.30m\n",
      "        21 217714368908.8815  -103268168.8418            1.25m\n",
      "        22 215675935701.6067  -134963791.2637            1.21m\n",
      "        23 212046333999.7946  -133703469.2647            1.17m\n",
      "        24 211893998434.2441  -106878442.3851            1.13m\n",
      "        25 212073378966.0103   -89654984.5830            1.09m\n",
      "        26 210287371724.6216  -129543531.8529            1.04m\n",
      "        27 208350792971.4646  -163199281.0521            1.00m\n",
      "        28 207737887942.9016  -172740595.9912           57.58s\n",
      "        29 206169112114.3193  -113881078.0799           55.05s\n",
      "        30 207188407792.1207   -83173282.6328           52.49s\n",
      "        31 204868091863.8637  -122938526.9856           49.91s\n",
      "        32 203159943862.1750  -141744770.8902           47.30s\n",
      "        33 204441353232.0168  -121960266.6122           44.71s\n",
      "        34 202651207140.3081  -104527622.3062           42.15s\n",
      "        35 202694316504.1966   -88028347.7424           39.54s\n",
      "        36 201557462938.7560  -123119719.1530           36.91s\n",
      "        37 200366230150.2874  -118772867.1874           34.26s\n",
      "        38 199996104145.2863  -101028965.0379           31.64s\n",
      "        39 199780832418.0392  -103973468.3589           29.04s\n",
      "        40 198238697350.4023  -133984232.0481           26.40s\n",
      "        41 198912993145.4350   -46853987.6720           23.76s\n",
      "        42 197335041292.2080  -103497901.6432           21.11s\n",
      "        43 195254009210.9828  -103428971.5673           18.47s\n",
      "        44 195040225635.9057  -220244026.1931           15.83s\n",
      "        45 195431884946.9202   -65012845.7965           13.19s\n",
      "        46 193545074473.2971  -173672778.3673           10.55s\n",
      "        47 195106250531.6459   -98583386.1987            7.91s\n",
      "        48 193312114182.8166  -124304560.7733            5.28s\n",
      "        49 192564506749.7375   -97725961.3210            2.64s\n",
      "        50 193907515063.8999   -67435695.3632            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 292990667263.8625  6942866350.1891            2.06m\n",
      "         2 283296952247.4081  5547143136.5396            2.02m\n",
      "         3 273710559696.1052  4536984287.6308            1.98m\n",
      "         4 267673685647.2517  3641061259.5265            1.93m\n",
      "         5 262701382694.0127  2887672442.4427            1.89m\n",
      "         6 256734637360.6957  2228226836.6287            1.86m\n",
      "         7 252131130061.9483  1856605116.0106            1.82m\n",
      "         8 247410383116.4558  1268948498.3033            1.77m\n",
      "         9 245537745684.1456  1026849232.0908            1.74m\n",
      "        10 239495501117.4290   852304752.6073            1.69m\n",
      "        11 238683380629.6733   567943168.6226            1.66m\n",
      "        12 234670837147.1760   401334962.3770            1.62m\n",
      "        13 232194076141.4706   301869425.6930            1.58m\n",
      "        14 229617241939.4678   287433960.8733            1.54m\n",
      "        15 227656255476.7852   167595617.8306            1.50m\n",
      "        16 224231863597.2722    81334685.3431            1.46m\n",
      "        17 225141210225.3445    47207628.9406            1.42m\n",
      "        18 221743580401.7181    40631989.0533            1.38m\n",
      "        19 219166521160.9018   -91796265.6689            1.34m\n",
      "        20 219314464143.8438   -52940126.3454            1.29m\n",
      "        21 218471216999.0156   -86927493.1061            1.25m\n",
      "        22 214870968988.0090  -100778759.3407            1.21m\n",
      "        23 214291900584.1119   -79681367.2128            1.17m\n",
      "        24 212824885103.6158  -137523394.2188            1.13m\n",
      "        25 213398375475.3837   -71194603.0708            1.08m\n",
      "        26 210766375574.3582  -122240786.9590            1.04m\n",
      "        27 210306603748.1945  -181385724.3259           59.97s\n",
      "        28 208779364852.3047  -167606362.0292           57.34s\n",
      "        29 207522414596.7982  -107387139.3959           54.77s\n",
      "        30 207564817191.7288  -184458549.7845           52.19s\n",
      "        31 206167866448.4392   -84952141.7340           49.58s\n",
      "        32 203148938240.4119  -100896285.3176           46.93s\n",
      "        33 204820933669.9797  -157305248.5797           44.39s\n",
      "        34 203857708719.8335   -71781296.4466           41.80s\n",
      "        35 201690667487.9066   -80431080.2288           39.21s\n",
      "        36 202079117530.8925  -123935923.3844           36.60s\n",
      "        37 200696838440.0280   -86658791.9425           33.98s\n",
      "        38 200810052001.2746   -98442644.8989           31.35s\n",
      "        39 200834269356.1024  -154777167.1734           28.76s\n",
      "        40 198612903836.2607   -94774241.8242           26.18s\n",
      "        41 197975549368.2104  -125889374.5147           23.58s\n",
      "        42 197047393875.4562   -98725823.7746           20.97s\n",
      "        43 196531129303.6729   -86953723.4957           18.36s\n",
      "        44 195397186857.7133  -129708555.4815           15.73s\n",
      "        45 193542634992.1763  -189467403.3199           13.12s\n",
      "        46 193979091988.2500   -77588218.0588           10.49s\n",
      "        47 194933347642.7762   -69421699.8776            7.87s\n",
      "        48 192565897554.3959  -134339573.9473            5.25s\n",
      "        49 193481802004.5730   -88926965.4201            2.63s\n",
      "        50 192787199400.6533   -92179946.5425            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = [\"r2\", \"neg_mean_absolute_percentage_error\"]\n",
    "\n",
    "GBReg = GradientBoostingRegressor(n_estimators=50, \n",
    "                                  max_depth = 15,\n",
    "                                  random_state = 101,\n",
    "                                  subsample = 0.5,\n",
    "                                  verbose = 2)\n",
    "\n",
    "scores = cross_validate(GBReg, X, Y,return_train_score = True, cv=5, scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16748ade-ecbd-4a72-aac9-78e12e112f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.37162326059931355\n",
      "Test: 0.09433785675070328\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_r2'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_r2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ed8acf-8810-445c-a265-dcb95eb23a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: -80.59527299850906\n",
      "Test: -103.76458592868335\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_neg_mean_absolute_percentage_error'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_neg_mean_absolute_percentage_error'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc22a3c-2334-4757-b4e8-9bfe8ca09c93",
   "metadata": {},
   "source": [
    "### Complicando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80bd6e3-4313-4534-bf94-42037890caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 293328834777.8557  6868297764.4179            6.38m\n",
      "         2 282251552499.8793  5596691742.6740            6.35m\n",
      "         3 272985398405.9755  4469513343.7328            6.30m\n",
      "         4 266890291158.3091  3487372101.7636            6.28m\n",
      "         5 261550704118.8408  2717967295.3863            6.24m\n",
      "         6 257113919193.4347  2203513059.5363            6.22m\n",
      "         7 250443577929.1233  1626320592.8499            6.18m\n",
      "         8 247589211523.8410  1313704592.7379            6.13m\n",
      "         9 243780491701.0064  1032608818.5385            6.09m\n",
      "        10 240293041235.9603   794652771.9281            6.03m\n",
      "        11 238789023675.6021   665334912.7422            5.99m\n",
      "        12 234054458652.4223   428513647.5253            5.96m\n",
      "        13 232545759780.4030   392383876.9779            5.93m\n",
      "        14 228897538868.1555   184477171.1641            5.90m\n",
      "        15 228485719517.0646   180352065.1932            5.85m\n",
      "        16 226312907367.3759    64971296.3837            5.81m\n",
      "        17 222566898100.1548   -80864527.0232            5.76m\n",
      "        18 220129774266.6416   -30699067.4675            5.72m\n",
      "        19 219730794290.2415   -26504381.3326            5.69m\n",
      "        20 217228511606.9101   -31448117.0344            5.66m\n",
      "        21 216682931147.2332   -92841075.7136            5.62m\n",
      "        22 215580838426.0231   -88796234.5006            5.58m\n",
      "        23 212197024352.6096  -123562598.5944            5.54m\n",
      "        24 211480020610.2064  -180206378.6266            5.51m\n",
      "        25 211801839403.8817  -151945479.8539            5.47m\n",
      "        26 210016238552.8531  -139545898.7332            5.43m\n",
      "        27 208076559828.2090  -158082733.1276            5.39m\n",
      "        28 208254270113.8438   -79890629.7280            5.35m\n",
      "        29 205316860986.7845  -123558044.6441            5.31m\n",
      "        30 206310832963.1176  -129135138.6261            5.27m\n",
      "        31 204664380163.3519  -114568620.9239            5.23m\n",
      "        32 204345065231.8499   -94345596.8227            5.20m\n",
      "        33 202802157595.8684   -93395220.7645            5.15m\n",
      "        34 201945073084.3999  -131974379.9076            5.11m\n",
      "        35 203153828036.0897  -127428105.1366            5.08m\n",
      "        36 201845279069.8014  -111448394.8906            5.04m\n",
      "        37 200935681232.5791   -94820674.0535            5.00m\n",
      "        38 200351472737.0050  -100321039.5613            4.96m\n",
      "        39 199943640611.5485  -142841453.2858            4.92m\n",
      "        40 197869556514.9556  -233389812.9319            4.87m\n",
      "        41 196890248640.4570  -105655357.9340            4.83m\n",
      "        42 197334805972.4277   -86700480.6245            4.78m\n",
      "        43 196378404512.9599   -61750100.3514            4.74m\n",
      "        44 195323033025.4159  -150243135.9679            4.69m\n",
      "        45 195390497104.6343   -80848004.1604            4.65m\n",
      "        46 194300476335.4195  -103424930.4283            4.61m\n",
      "        47 192808871352.9602  -109626882.1590            4.57m\n",
      "        48 193777960660.5766   -94701322.9937            4.52m\n",
      "        49 192186513136.9339  -110704135.8999            4.48m\n",
      "        50 191629901971.3315   -91850661.3258            4.43m\n",
      "        51 191101457933.5993  -115695816.1868            4.39m\n",
      "        52 190025386531.0952  -105188495.6940            4.35m\n",
      "        53 190693799231.2536   -80313700.7097            4.31m\n",
      "        54 188402107941.5006  -105156038.2061            4.27m\n",
      "        55 189723480316.1248   -63646992.6380            4.22m\n",
      "        56 188617307860.4104  -131229850.4924            4.18m\n",
      "        57 187704182599.6459   -81642088.1589            4.13m\n",
      "        58 187356899666.7171  -141562972.6409            4.09m\n",
      "        59 185954502727.5175  -120100850.5728            4.05m\n",
      "        60 186483574382.2821   -70980947.3783            4.00m\n",
      "        61 186444402007.8473   -54043987.7445            3.96m\n",
      "        62 185643640886.2686   -87195698.5921            3.92m\n",
      "        63 183674030313.5850  -133764116.3680            3.87m\n",
      "        64 184805263250.0931   -72287864.5045            3.83m\n",
      "        65 184252173738.8096   -76051718.0535            3.78m\n",
      "        66 182194870538.7559   -66502908.8055            3.74m\n",
      "        67 182493376970.7829  -126120075.2622            3.69m\n",
      "        68 180987135215.1979  -119848270.9991            3.65m\n",
      "        69 179946380565.1013  -132355623.5112            3.61m\n",
      "        70 179369305519.0258   -82690188.7249            3.56m\n",
      "        71 178893188958.9415  -143875447.4854            3.52m\n",
      "        72 179325292533.8873   -88930285.2003            3.47m\n",
      "        73 178982166854.1208   -78681225.7625            3.43m\n",
      "        74 177270281440.2764  -173168347.6348            3.39m\n",
      "        75 177492608323.5938  -123437896.5979            3.34m\n",
      "        76 176188535929.6968  -124674803.2592            3.30m\n",
      "        77 173925187309.0041  -208744643.7190            3.25m\n",
      "        78 173691357125.2727  -104806906.9220            3.21m\n",
      "        79 174569799451.1515  -102072702.5520            3.16m\n",
      "        80 174160870346.4017   -64048528.2532            3.12m\n",
      "        81 173628441548.6606   -64298885.1727            3.07m\n",
      "        82 173814177588.1206   -75667843.9157            3.03m\n",
      "        83 171101159346.5038  -116781325.7981            2.98m\n",
      "        84 172771175825.3029   -60168971.5832            2.94m\n",
      "        85 170940599398.5399   -98410470.1686            2.89m\n",
      "        86 170917181751.6736   -77691381.4316            2.85m\n",
      "        87 170394871986.9000  -102164304.1140            2.80m\n",
      "        88 169148561808.2418  -116279889.8324            2.76m\n",
      "        89 168902706326.7947  -114027878.9863            2.71m\n",
      "        90 167458155511.2044  -134734888.1509            2.67m\n",
      "        91 168108115593.4179   -88611100.3710            2.62m\n",
      "        92 168619511758.6637   -47256435.0627            2.58m\n",
      "        93 168242570765.6552   -63448693.7112            2.54m\n",
      "        94 166164477885.6265  -104061561.0318            2.49m\n",
      "        95 167076354314.9091   -54034722.7863            2.45m\n",
      "        96 165983126219.0139   -55529930.4430            2.40m\n",
      "        97 164871276948.5476   -46214591.0063            2.36m\n",
      "        98 165275249186.2596  -115564681.3887            2.31m\n",
      "        99 164040425216.8821  -122669559.8812            2.27m\n",
      "       100 163760273373.5945  -100215070.6742            2.23m\n",
      "       101 164230278345.3364  -106577793.7070            2.18m\n",
      "       102 162563327267.1688   -82300221.9402            2.14m\n",
      "       103 162562975864.3631   -70799598.0569            2.09m\n",
      "       104 162086570540.1687   -64132104.0529            2.05m\n",
      "       105 162809683573.7848   -92171957.1628            2.01m\n",
      "       106 161703613882.2928  -120330128.8293            1.96m\n",
      "       107 159812906214.7809  -125224249.0789            1.92m\n",
      "       108 159156258138.5404   -83179312.0534            1.87m\n",
      "       109 160640024940.7415   -41722918.3653            1.83m\n",
      "       110 159685411111.7528   -98293317.4977            1.78m\n",
      "       111 159807165986.9876   -86740189.7064            1.74m\n",
      "       112 158556818495.4991  -122091402.7778            1.69m\n",
      "       113 158178157178.1906   -57994779.2533            1.65m\n",
      "       114 158431978354.1011   -65864806.3524            1.61m\n",
      "       115 157018622088.9141  -106424725.8956            1.56m\n",
      "       116 156821738634.3467   -77996421.9073            1.52m\n",
      "       117 156247021322.5822   -87000957.4810            1.47m\n",
      "       118 156799693179.3013   -52039775.0085            1.43m\n",
      "       119 155887704414.8921   -44284953.9852            1.38m\n",
      "       120 156113155520.8306   -86389885.3274            1.34m\n",
      "       121 154427201189.4076   -84418591.2479            1.29m\n",
      "       122 154264588145.7027  -168676779.5892            1.25m\n",
      "       123 152496201715.0487   -53788303.7481            1.21m\n",
      "       124 154229037774.2791   -60570586.2213            1.16m\n",
      "       125 153157645815.6060   -79754141.3794            1.12m\n",
      "       126 152367571113.2142   -97600751.7446            1.07m\n",
      "       127 153008574686.9897   -52886155.2472            1.03m\n",
      "       128 151460465305.0009  -114687763.6766           58.98s\n",
      "       129 150985152965.4886   -75823227.4934           56.28s\n",
      "       130 151595477369.2769   -38484299.2221           53.61s\n",
      "       131 150865449414.2759  -131176558.5131           50.95s\n",
      "       132 148258923615.6177  -162418600.9142           48.26s\n",
      "       133 149277554358.8036   -51695630.3254           45.58s\n",
      "       134 148333545397.7248  -162271006.2224           42.89s\n",
      "       135 148742184978.4687   -69558683.8977           40.21s\n",
      "       136 148420990513.8590   -59787533.9603           37.53s\n",
      "       137 147269227310.6005  -106313512.6460           34.85s\n",
      "       138 148391176004.3779   -81161954.2495           32.18s\n",
      "       139 146056022712.5948  -140257415.3068           29.49s\n",
      "       140 145226751840.2856  -145925158.3835           26.81s\n",
      "       141 145910164330.3941   -53756515.9102           24.13s\n",
      "       142 144801402680.5905   -70734929.9683           21.44s\n",
      "       143 144814461788.8214   -90452439.6499           18.76s\n",
      "       144 143852202906.7375  -107576735.1950           16.08s\n",
      "       145 144836080600.6482   -67687241.0390           13.40s\n",
      "       146 142694057610.5407   -55980140.8535           10.72s\n",
      "       147 143648409402.7684   -68042170.7689            8.04s\n",
      "       148 142778095152.0576   -63053597.2227            5.36s\n",
      "       149 141891125392.6362  -100718218.1618            2.68s\n",
      "       150 143255791144.6270  -116932557.8881            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 294444311896.4337  6939707679.1937            6.39m\n",
      "         2 283501971902.0312  5600238865.8167            6.34m\n",
      "         3 272697016464.7031  4406743125.3911            6.25m\n",
      "         4 266608605584.0973  3581875180.4048            6.20m\n",
      "         5 262901913236.4018  2698713797.6807            6.15m\n",
      "         6 256403430941.9872  2228713824.8874            6.12m\n",
      "         7 250456248337.3235  1716450279.5630            6.09m\n",
      "         8 246635328595.0633  1385230040.4796            6.04m\n",
      "         9 244546018634.0053  1098949712.3544            6.01m\n",
      "        10 240549499644.1192   705117216.1910            5.96m\n",
      "        11 237758595171.3362   614622296.5792            5.91m\n",
      "        12 233701816444.2810   441740899.5597            5.88m\n",
      "        13 231022280138.7484   373328319.1832            5.86m\n",
      "        14 228653762808.1043   256579074.7902            5.82m\n",
      "        15 227605841598.6737   147580545.4981            5.78m\n",
      "        16 225878374696.6058    44631438.2593            5.76m\n",
      "        17 224058564736.3160     9885885.4433            5.72m\n",
      "        18 221836391300.3033    -1061313.1843            5.69m\n",
      "        19 220354684774.7640   -32175382.8627            5.66m\n",
      "        20 217585753051.1588   -99916484.8432            5.63m\n",
      "        21 217664251945.7922   -66062174.0023            5.59m\n",
      "        22 215903310185.9330  -170789267.3772            5.55m\n",
      "        23 213978964768.8767  -171244292.4169            5.52m\n",
      "        24 211229621771.3873  -209586035.0220            5.48m\n",
      "        25 212997588146.1358  -123834728.7143            5.45m\n",
      "        26 210180175806.3175   -77889031.0630            5.41m\n",
      "        27 208779500064.8437  -153232612.1735            5.37m\n",
      "        28 208446609625.3075  -175019681.1237            5.33m\n",
      "        29 205330700988.6145   -86091307.0304            5.29m\n",
      "        30 207350190963.6458   -98704045.6775            5.25m\n",
      "        31 204257614100.1859  -160040721.6539            5.21m\n",
      "        32 204343388903.2912   -82040707.1752            5.17m\n",
      "        33 202822904308.8700  -145162903.8181            5.13m\n",
      "        34 201816642501.6136   -75533535.8568            5.09m\n",
      "        35 202733169290.2716  -153393975.6681            5.06m\n",
      "        36 199198943598.3565  -188206393.7033            5.01m\n",
      "        37 199603138865.5178   -63697167.8414            4.97m\n",
      "        38 198945622146.1350  -116319010.7769            4.93m\n",
      "        39 199837152359.0236  -102528772.7273            4.89m\n",
      "        40 199740541942.3214  -103933568.4970            4.85m\n",
      "        41 197786668542.3620   -61889835.3937            4.81m\n",
      "        42 197444696420.7966   -84867452.6595            4.77m\n",
      "        43 195274913670.1811  -167208387.5318            4.73m\n",
      "        44 195984916831.2281   -73937679.0382            4.69m\n",
      "        45 195364259215.0422   -72703776.5952            4.64m\n",
      "        46 193275490538.9714  -134725294.6175            4.60m\n",
      "        47 194661845031.7537   -75364189.4287            4.56m\n",
      "        48 193082289174.8018  -160938691.3867            4.52m\n",
      "        49 192692632672.2472  -130116132.4656            4.47m\n",
      "        50 192368485072.4443   -60590135.4215            4.43m\n",
      "        51 190436182705.1363  -124257614.9805            4.39m\n",
      "        52 189289264771.4236  -132248006.1628            4.35m\n",
      "        53 189153294617.0103  -173821105.5744            4.30m\n",
      "        54 187890827735.5164   -90609128.5901            4.26m\n",
      "        55 188404166735.3489  -110976749.2244            4.21m\n",
      "        56 187632616955.0973  -102748606.4462            4.17m\n",
      "        57 187197386075.6816   -98027152.2391            4.12m\n",
      "        58 186752973912.3581   -92515978.3140            4.08m\n",
      "        59 185031583883.6051  -159270913.5089            4.04m\n",
      "        60 185657400674.5815   -72071729.8627            3.99m\n",
      "        61 185117991918.1725   -99945205.4622            3.95m\n",
      "        62 183402431159.8416   -97838444.4346            3.90m\n",
      "        63 183467740457.3259   -78366534.3926            3.86m\n",
      "        64 183455833467.9189   -99729742.0876            3.81m\n",
      "        65 183204739225.9261   -98661474.7146            3.77m\n",
      "        66 181238880789.6295   -68614788.2898            3.72m\n",
      "        67 182893480762.9327   -58542448.8814            3.68m\n",
      "        68 179527550928.5858  -182353132.7629            3.64m\n",
      "        69 180135150043.9520  -108791497.6116            3.59m\n",
      "        70 178808471662.3340   -83828178.9797            3.55m\n",
      "        71 178535421227.7411  -157498003.6582            3.50m\n",
      "        72 178289577751.8464   -79665199.5598            3.46m\n",
      "        73 178011359396.2556  -117059932.1822            3.41m\n",
      "        74 176650270467.1018  -101542108.5369            3.37m\n",
      "        75 177451404667.3295   -54431085.0984            3.32m\n",
      "        76 176317536525.6953  -127133509.7853            3.28m\n",
      "        77 176773308596.5993   -59656949.5722            3.23m\n",
      "        78 175436405769.3275   -71819291.8173            3.19m\n",
      "        79 175368400220.4374   -93920219.3463            3.15m\n",
      "        80 175110110107.6221   -69647066.9348            3.10m\n",
      "        81 175161957715.0558   -54180366.6684            3.06m\n",
      "        82 175115753747.3647   -68410075.3836            3.01m\n",
      "        83 172581214963.0627   -61917234.7796            2.96m\n",
      "        84 174565519623.4729   -53743437.4436            2.92m\n",
      "        85 172394706112.6311  -121661816.6988            2.87m\n",
      "        86 171710971485.8160  -107700791.5249            2.83m\n",
      "        87 171340498520.8758  -104280575.1933            2.78m\n",
      "        88 170771635663.3717  -106810680.6060            2.74m\n",
      "        89 170123898992.1285  -114768361.0941            2.69m\n",
      "        90 168159018597.8748  -128116876.0489            2.65m\n",
      "        91 169478582507.6195  -102976691.6255            2.61m\n",
      "        92 169310141865.8704   -54019949.5926            2.56m\n",
      "        93 168854762843.7917   -85786651.9634            2.52m\n",
      "        94 166994311845.8218  -141756875.0211            2.48m\n",
      "        95 167715856656.3830   -70708427.8718            2.43m\n",
      "        96 166468244194.3221  -130501930.5399            2.39m\n",
      "        97 165950738351.5969   -79376632.5616            2.34m\n",
      "        98 166335314940.3329   -93913532.1646            2.30m\n",
      "        99 165775477841.9103  -122584789.4478            2.26m\n",
      "       100 164252342030.5856   -90117996.1255            2.21m\n",
      "       101 164808337105.0847   -81064509.0363            2.17m\n",
      "       102 163066642493.7017  -114697239.6892            2.12m\n",
      "       103 163379649018.5548   -80960137.8174            2.08m\n",
      "       104 162266617945.0312  -109429288.6208            2.03m\n",
      "       105 163219487694.0551   -67660649.2119            1.99m\n",
      "       106 162420450735.1340  -101632424.3433            1.95m\n",
      "       107 161761306040.9598   -35371514.2446            1.90m\n",
      "       108 160542631137.9820   -92635865.5215            1.86m\n",
      "       109 161079399918.5987   -87739636.2377            1.81m\n",
      "       110 160069572205.3301   -95565267.7651            1.77m\n",
      "       111 160502681473.6701  -122091449.4460            1.72m\n",
      "       112 160219772842.9852   -92476448.4333            1.68m\n",
      "       113 158709820385.3360  -103159776.1403            1.64m\n",
      "       114 158311918407.1861  -113144623.9922            1.59m\n",
      "       115 157961551776.2247  -122187933.1655            1.55m\n",
      "       116 156665682112.0272  -172864652.3585            1.50m\n",
      "       117 157050785915.3212   -72126334.3704            1.46m\n",
      "       118 156075323024.5043   -70124835.7327            1.42m\n",
      "       119 156167650705.2115  -119882273.0528            1.37m\n",
      "       120 155705440671.5219  -100695592.9412            1.33m\n",
      "       121 154773465179.1987   -84324238.2178            1.28m\n",
      "       122 154820935605.0927   -53116794.0594            1.24m\n",
      "       123 154096931936.9448   -66200580.6771            1.20m\n",
      "       124 154525330388.0800   -57328350.4836            1.15m\n",
      "       125 153101818404.6349  -131537209.3109            1.11m\n",
      "       126 151696499824.2758  -117158090.4713            1.06m\n",
      "       127 152695893770.4174   -52167698.7227            1.02m\n",
      "       128 151397643447.0301  -100580358.9638           58.49s\n",
      "       129 151772013590.7273   -86240074.3450           55.82s\n",
      "       130 151885055219.2428   -42141515.2900           53.18s\n",
      "       131 151175391597.6840   -84935551.2512           50.52s\n",
      "       132 149347931508.9377  -102863309.7604           47.87s\n",
      "       133 148924741784.3047   -93014704.1870           45.22s\n",
      "       134 149012265404.2377  -101007000.7985           42.55s\n",
      "       135 148679433916.8408   -87042954.2665           39.88s\n",
      "       136 149081009870.7968   -69195234.3832           37.24s\n",
      "       137 148133308528.5518   -41882238.9088           34.58s\n",
      "       138 148371374744.9891   -93845234.5776           31.91s\n",
      "       139 146898087500.8842  -119576780.2928           29.26s\n",
      "       140 146585187661.6815   -66728280.4448           26.60s\n",
      "       141 146943372130.6907   -34956857.5695           23.93s\n",
      "       142 145607731039.5332   -86295182.6068           21.27s\n",
      "       143 145468858439.0635   -78285323.6757           18.62s\n",
      "       144 145438548250.2160   -78258826.1128           15.96s\n",
      "       145 145364070731.6357   -89649668.7251           13.30s\n",
      "       146 143418833925.5338   -97210163.0183           10.63s\n",
      "       147 144340172136.4555   -64784677.8459            7.98s\n",
      "       148 143209751667.8238  -144772582.1464            5.32s\n",
      "       149 142738622271.4567   -64947773.5234            2.66s\n",
      "       150 144192679310.6221   -69388617.1132            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 294464536154.8582  6980160905.9633            6.32m\n",
      "         2 282044485665.4380  5448760149.7598            6.32m\n",
      "         3 272972464148.8442  4354873647.1646            6.27m\n",
      "         4 267533398816.8171  3443093292.5025            6.24m\n",
      "         5 261806273727.9099  2829072693.9544            6.17m\n",
      "         6 255954308627.7593  2218232383.4889            6.11m\n",
      "         7 251776636654.3162  1886173078.8391            6.08m\n",
      "         8 247113072618.9442  1355689367.9578            6.05m\n",
      "         9 245434263801.4573  1062623112.5441            6.03m\n",
      "        10 240397811384.2138   738530591.8497            6.00m\n",
      "        11 238737930500.6208   548841569.9097            5.95m\n",
      "        12 233834889933.3976   456228939.2904            5.92m\n",
      "        13 231261533858.7744   302678299.0480            5.88m\n",
      "        14 230330477971.0142   206632840.2026            5.85m\n",
      "        15 228229364432.3605   124125782.5621            5.82m\n",
      "        16 225637925075.2156    50930278.9143            5.77m\n",
      "        17 224754261368.5739    65816476.1493            5.73m\n",
      "        18 221575712113.5230    -4414809.3357            5.70m\n",
      "        19 219976009085.9357   -54665155.1404            5.67m\n",
      "        20 218866525396.9316  -100002636.6708            5.63m\n",
      "        21 218212296376.9165   -62835899.3350            5.59m\n",
      "        22 216882752470.6010  -127219170.3950            5.56m\n",
      "        23 213281061732.3946  -162215217.1335            5.52m\n",
      "        24 212352894420.6817  -169407980.9979            5.48m\n",
      "        25 211878210415.1266  -153614918.7209            5.44m\n",
      "        26 210506500491.1638  -113196219.3751            5.40m\n",
      "        27 209000655231.1433  -100963701.9763            5.36m\n",
      "        28 209226954990.3818  -129374723.1768            5.32m\n",
      "        29 206234739045.1099   -97216647.7274            5.29m\n",
      "        30 207681612354.7837   -89641433.6310            5.25m\n",
      "        31 205169022556.0153  -161400285.2149            5.21m\n",
      "        32 202880224506.1103  -180449437.3004            5.16m\n",
      "        33 203239397812.5652  -119482586.1516            5.11m\n",
      "        34 202275272031.7698   -86770521.9710            5.07m\n",
      "        35 200650655097.2983  -215889006.0971            5.03m\n",
      "        36 200261324347.1180  -102109788.1339            4.99m\n",
      "        37 199461390919.3315   -60098326.6875            4.95m\n",
      "        38 199645727061.2871   -96056747.9160            4.90m\n",
      "        39 199247656850.5834  -148059274.5127            4.86m\n",
      "        40 198616287195.4014  -149258527.4265            4.82m\n",
      "        41 198167153823.6283   -82704618.9459            4.78m\n",
      "        42 196711435451.9860  -113379699.3075            4.74m\n",
      "        43 195069342148.4355  -198993104.9672            4.69m\n",
      "        44 193318475354.2018  -240260372.5256            4.65m\n",
      "        45 193626480669.9963  -122689405.4214            4.60m\n",
      "        46 192879618248.5894   -30311402.6636            4.57m\n",
      "        47 193667341100.6326   -79562121.7655            4.52m\n",
      "        48 191331839335.1210  -184921394.0080            4.48m\n",
      "        49 191585736381.3359   -67943601.5300            4.44m\n",
      "        50 191762374857.1431   -71847737.1821            4.39m\n",
      "        51 190067732841.2337  -121072311.7715            4.35m\n",
      "        52 188032820413.6227  -145097061.8014            4.31m\n",
      "        53 188389182265.5443   -62869694.9420            4.26m\n",
      "        54 187052447529.9066   -95586238.7470            4.22m\n",
      "        55 187245991481.5595  -121391547.1461            4.18m\n",
      "        56 186706419778.2755   -39960671.5772            4.13m\n",
      "        57 185692342236.6611  -158496135.3840            4.09m\n",
      "        58 185956040770.0738  -106333550.1691            4.04m\n",
      "        59 184592234602.3425  -124997040.3288            4.00m\n",
      "        60 184695775864.7494  -147417474.9290            3.95m\n",
      "        61 182666359618.5465  -113050070.0409            3.91m\n",
      "        62 182175370548.7005  -123265481.4993            3.87m\n",
      "        63 182254378799.6986   -98738453.4466            3.82m\n",
      "        64 182008920153.6161   -84968016.0860            3.78m\n",
      "        65 181656842653.1266  -110009830.7009            3.74m\n",
      "        66 179904453511.6638  -155038662.2191            3.69m\n",
      "        67 180624174180.0570  -106219084.5689            3.64m\n",
      "        68 178010644251.5876  -124570779.7874            3.60m\n",
      "        69 177623178336.0836  -103569704.2439            3.56m\n",
      "        70 176908662847.0653  -126151474.7165            3.51m\n",
      "        71 176479729322.0967  -113876388.3063            3.47m\n",
      "        72 176221068737.8365   -40482234.4181            3.42m\n",
      "        73 176840229289.7687   -89459853.4001            3.38m\n",
      "        74 175509023911.0930  -123488471.6934            3.33m\n",
      "        75 174825153915.0559  -117624923.1810            3.29m\n",
      "        76 175464057706.2770   -84034711.0321            3.25m\n",
      "        77 174345044365.7914   -69004007.1276            3.20m\n",
      "        78 173034986239.1683   -84366133.2903            3.16m\n",
      "        79 173293690622.2548   -85640214.2585            3.11m\n",
      "        80 172934780662.3488   -69647242.2163            3.07m\n",
      "        81 173032674901.3206   -63553027.2701            3.02m\n",
      "        82 172073776538.3327  -109461971.2903            2.98m\n",
      "        83 170719927291.5845  -107238627.3364            2.93m\n",
      "        84 172151931983.5788   -70334253.9814            2.89m\n",
      "        85 170999358798.2595   -90562383.4454            2.85m\n",
      "        86 169918580586.4364   -40374188.7679            2.80m\n",
      "        87 168419369931.8937  -154301450.8879            2.76m\n",
      "        88 169562990388.3904   -49435225.7252            2.71m\n",
      "        89 168191548899.4185   -96786470.0558            2.67m\n",
      "        90 166559528002.2274  -130725309.4807            2.62m\n",
      "        91 167164388391.4188  -164410221.9126            2.58m\n",
      "        92 167605790284.0350   -68515208.1602            2.54m\n",
      "        93 166350188909.0174   -83988644.4384            2.50m\n",
      "        94 165250005840.9357  -128006101.8842            2.45m\n",
      "        95 164619188765.1744   -94500974.1331            2.41m\n",
      "        96 165194420888.5254   -77994834.0917            2.37m\n",
      "        97 164292971788.4115   -64682230.8846            2.32m\n",
      "        98 164273263447.6206  -101046059.7281            2.28m\n",
      "        99 164364128677.8302  -100754667.4276            2.24m\n",
      "       100 162016685590.7176   -99324791.7162            2.19m\n",
      "       101 162468325187.0877   -66571661.4330            2.15m\n",
      "       102 161706681523.7269   -96899228.2555            2.10m\n",
      "       103 161701646262.2696   -46545697.1431            2.06m\n",
      "       104 159718153707.1659  -113139085.5019            2.02m\n",
      "       105 160045983312.6242  -172491048.8484            1.97m\n",
      "       106 159080997321.6161  -111925227.1166            1.93m\n",
      "       107 158374736230.8094  -104337061.1798            1.88m\n",
      "       108 158250339653.7191   -52112092.0296            1.84m\n",
      "       109 158050837344.1381   -86593088.9360            1.80m\n",
      "       110 157342225477.0385  -100887993.3836            1.75m\n",
      "       111 157514632861.6908  -114166932.0340            1.71m\n",
      "       112 156433049099.7022  -124908724.1556            1.67m\n",
      "       113 155110330957.1642  -131114596.9148            1.62m\n",
      "       114 155195746670.6890   -27407681.8402            1.58m\n",
      "       115 155803485068.1227   -64157465.5853            1.54m\n",
      "       116 153527774727.9870  -101265099.2078            1.49m\n",
      "       117 153891955011.6062   -79152007.4358            1.45m\n",
      "       118 153681009689.4653   -62272589.1854            1.40m\n",
      "       119 153793947045.9508   -70887004.4952            1.36m\n",
      "       120 152449414612.8173  -124054957.9528            1.32m\n",
      "       121 151893674440.3688   -66106983.6078            1.27m\n",
      "       122 151420276178.4454  -131299569.5987            1.23m\n",
      "       123 151875200730.9105  -107227763.7560            1.19m\n",
      "       124 151611805605.8891   -43932136.4551            1.14m\n",
      "       125 151403170785.3340   -67648563.1929            1.10m\n",
      "       126 149619497182.6103   -46963237.2987            1.05m\n",
      "       127 150913466139.1472   -55771011.0436            1.01m\n",
      "       128 149622593596.3826  -136473566.0589           57.98s\n",
      "       129 148559357581.0815   -77989374.2866           55.36s\n",
      "       130 148874413740.5263   -65547941.6391           52.74s\n",
      "       131 148849709881.6829   -66941702.2851           50.09s\n",
      "       132 145682307912.1367  -196493708.5572           47.45s\n",
      "       133 145914483360.8614   -79766234.5497           44.81s\n",
      "       134 147493974660.4329   -61097403.3589           42.18s\n",
      "       135 145752966101.3917   -98897206.9897           39.54s\n",
      "       136 144880836251.7613   -91479718.3296           36.91s\n",
      "       137 144658136648.6945   -86255825.9466           34.28s\n",
      "       138 145896567431.9892   -70000799.7677           31.65s\n",
      "       139 144314490567.3069   -76541687.4250           29.03s\n",
      "       140 144686057410.0828  -108926372.9165           26.39s\n",
      "       141 143512073963.6873   -83602628.2468           23.75s\n",
      "       142 142837332433.7055   -80585963.1394           21.12s\n",
      "       143 142153495686.3508   -82324908.1078           18.48s\n",
      "       144 141588228878.4106  -139317145.9836           15.84s\n",
      "       145 141883425871.3841   -77649839.4542           13.20s\n",
      "       146 140540287770.6263  -134815672.4763           10.56s\n",
      "       147 140776012741.9041  -113268673.1423            7.92s\n",
      "       148 139504659621.9976  -107721044.3319            5.28s\n",
      "       149 139483018332.8941   -83607474.8528            2.64s\n",
      "       150 140539050088.6569   -67223876.8391            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 293083173686.7731  6862054565.2655            6.27m\n",
      "         2 281246589956.2083  5499640494.2556            6.35m\n",
      "         3 272943538191.9362  4422257089.1102            6.30m\n",
      "         4 267159934660.9238  3585347816.8026            6.26m\n",
      "         5 262424757218.9370  2838361969.9956            6.22m\n",
      "         6 255683734261.4362  2261892913.9675            6.16m\n",
      "         7 250771255380.5904  1774590320.1316            6.10m\n",
      "         8 246533835604.0155  1405120420.6857            6.08m\n",
      "         9 244799563036.7750  1096830515.6915            6.05m\n",
      "        10 238863020179.1101   765282391.4133            6.02m\n",
      "        11 238047280442.6553   540974049.3720            5.98m\n",
      "        12 232818973857.9498   529016506.5146            5.93m\n",
      "        13 232649086647.4088   323489704.2069            5.90m\n",
      "        14 229375456635.8850   143020305.3643            5.87m\n",
      "        15 227124584365.4470    54052553.6515            5.84m\n",
      "        16 225101532205.9782   121746483.3338            5.80m\n",
      "        17 224859169851.1905   -25640296.3132            5.76m\n",
      "        18 221652548780.2383   -43292848.3352            5.71m\n",
      "        19 218949715737.5374    14767429.2681            5.68m\n",
      "        20 218726303452.2562   -80386733.9995            5.64m\n",
      "        21 217714368908.8815  -103268168.8418            5.60m\n",
      "        22 215675935701.6067  -134963791.2637            5.56m\n",
      "        23 212046333999.7946  -133703469.2647            5.51m\n",
      "        24 211893998434.2441  -106878442.3851            5.48m\n",
      "        25 212073378966.0103   -89654984.5830            5.44m\n",
      "        26 210287371724.6216  -129543531.8529            5.40m\n",
      "        27 208350792971.4646  -163199281.0521            5.37m\n",
      "        28 207737887942.9016  -172740595.9912            5.32m\n",
      "        29 206169112114.3193  -113881078.0799            5.29m\n",
      "        30 207188407792.1207   -83173282.6328            5.25m\n",
      "        31 204868091863.8637  -122938526.9856            5.21m\n",
      "        32 203159943862.1750  -141744770.8902            5.17m\n",
      "        33 204441353232.0168  -121960266.6122            5.13m\n",
      "        34 202651207140.3081  -104527622.3062            5.09m\n",
      "        35 202694316504.1966   -88028347.7424            5.05m\n",
      "        36 201557462938.7560  -123119719.1530            5.01m\n",
      "        37 200366230150.2874  -118772867.1874            4.97m\n",
      "        38 199996104145.2863  -101028965.0379            4.93m\n",
      "        39 199780832418.0392  -103973468.3589            4.89m\n",
      "        40 198238697350.4023  -133984232.0481            4.85m\n",
      "        41 198912993145.4350   -46853987.6720            4.80m\n",
      "        42 197335041292.2080  -103497901.6432            4.76m\n",
      "        43 195254009210.9828  -103428971.5673            4.72m\n",
      "        44 195040225635.9057  -220244026.1931            4.67m\n",
      "        45 195431884946.9202   -65012845.7965            4.62m\n",
      "        46 193545074473.2971  -173672778.3673            4.58m\n",
      "        47 195106250531.6459   -98583386.1987            4.53m\n",
      "        48 193312114182.8166  -124304560.7733            4.49m\n",
      "        49 192564506749.7375   -97725961.3210            4.45m\n",
      "        50 193907515063.8999   -67435695.3632            4.41m\n",
      "        51 191665137358.9629  -141806535.6340            4.36m\n",
      "        52 190597226731.7021  -160349108.8408            4.32m\n",
      "        53 189859451075.5883   -44371314.6115            4.27m\n",
      "        54 189624688850.7224   -56938970.2289            4.23m\n",
      "        55 189767987658.9731   -86774187.2270            4.19m\n",
      "        56 189029266839.1104   -94417119.9110            4.14m\n",
      "        57 187909629182.8116  -153233757.3297            4.10m\n",
      "        58 188233550725.6319   -94676948.8836            4.05m\n",
      "        59 187421088186.3582  -156862470.2539            4.01m\n",
      "        60 188175637343.7633   -63732917.5563            3.97m\n",
      "        61 185376253035.9297   -77898318.7821            3.92m\n",
      "        62 184861666701.0586   -80853658.9930            3.87m\n",
      "        63 184647197571.0501   -69295283.4973            3.83m\n",
      "        64 185420035759.0722   -81724631.4072            3.79m\n",
      "        65 184186126700.3494  -117846236.2467            3.75m\n",
      "        66 183781194749.7509   -95792238.2075            3.70m\n",
      "        67 183877068615.1598   -67316298.1221            3.66m\n",
      "        68 183085325269.9057   -83256461.2825            3.61m\n",
      "        69 182290191502.3340   -83205400.0050            3.57m\n",
      "        70 182068016870.3847  -119046917.8699            3.52m\n",
      "        71 180984012676.0042   -95947812.3110            3.48m\n",
      "        72 181177152657.6115   -68753384.0911            3.44m\n",
      "        73 180821357694.5441   -96622980.0349            3.39m\n",
      "        74 179120992524.0075  -122321672.8401            3.35m\n",
      "        75 179367334178.2886   -87710492.0074            3.30m\n",
      "        76 179544183365.4630   -66039721.1766            3.26m\n",
      "        77 177889376052.4346  -117422369.9949            3.22m\n",
      "        78 177415607965.2436   -75055662.7220            3.17m\n",
      "        79 176896712916.9777   -58370933.2202            3.12m\n",
      "        80 176912519339.2199   -87695890.0072            3.08m\n",
      "        81 177234457240.8806   -83690003.8971            3.04m\n",
      "        82 176446463867.7446  -111645424.9473            2.99m\n",
      "        83 174870201898.5296   -99298550.1611            2.95m\n",
      "        84 174656676530.0336  -113119564.7966            2.91m\n",
      "        85 173875389413.1722   -82736307.8937            2.86m\n",
      "        86 172752891502.0159  -145139065.2432            2.82m\n",
      "        87 171679426165.1172  -105469315.7348            2.78m\n",
      "        88 171785902292.8083  -131349839.6869            2.73m\n",
      "        89 171117790106.0967  -150974171.2813            2.69m\n",
      "        90 169506884340.0028  -119603074.4109            2.64m\n",
      "        91 170798075323.3767   -80098086.2368            2.60m\n",
      "        92 169672983324.5084   -69367091.4849            2.56m\n",
      "        93 169567237715.3778   -87546617.5568            2.51m\n",
      "        94 168879151204.7239  -100058616.8238            2.47m\n",
      "        95 168062558300.6784   -92636679.7866            2.43m\n",
      "        96 168043285474.4659   -72733665.1193            2.38m\n",
      "        97 167668047645.5243   -73480757.7250            2.34m\n",
      "        98 167083503801.3075  -131817799.5980            2.29m\n",
      "        99 167463252192.2880   -99609074.9445            2.25m\n",
      "       100 165865969283.6147  -131232257.6826            2.21m\n",
      "       101 164595529145.9154  -132878165.5184            2.16m\n",
      "       102 164553848841.8235   -72237464.9588            2.12m\n",
      "       103 164583062453.3584   -86789854.4746            2.07m\n",
      "       104 162831216210.6891   -97286032.8459            2.03m\n",
      "       105 164391473145.7961   -22376057.8285            1.99m\n",
      "       106 163656400664.1384   -78153491.5394            1.94m\n",
      "       107 163020961418.2657   -40889071.5696            1.90m\n",
      "       108 162599580204.6653  -100139791.4565            1.85m\n",
      "       109 161977799932.8448   -88207871.0412            1.81m\n",
      "       110 161150912013.6836  -147509086.1624            1.77m\n",
      "       111 162209521650.8511   -38203965.8445            1.72m\n",
      "       112 160578677716.4460   -61243463.0485            1.68m\n",
      "       113 159820624563.6133  -106762374.6712            1.63m\n",
      "       114 158669310681.5671   -88846178.8956            1.59m\n",
      "       115 159266310904.7257   -92858759.9214            1.55m\n",
      "       116 158781874875.6893   -61932455.1233            1.50m\n",
      "       117 158342927167.5514   -86302871.3668            1.46m\n",
      "       118 157211501118.0355  -118704615.4698            1.41m\n",
      "       119 157742365162.5594   -62090479.9114            1.37m\n",
      "       120 157273378005.1748   -98954408.9331            1.32m\n",
      "       121 156300650314.0424  -103445497.9319            1.28m\n",
      "       122 155913919781.9164   -53810452.9492            1.24m\n",
      "       123 154859059710.5074  -123909030.2559            1.19m\n",
      "       124 154580671820.2380  -103400544.4207            1.15m\n",
      "       125 155203394169.8172  -121013218.0920            1.10m\n",
      "       126 153488860665.9529   -57378493.8242            1.06m\n",
      "       127 153854545429.0977  -101291911.3263            1.01m\n",
      "       128 152542878673.3751  -116618737.4174           58.20s\n",
      "       129 151759397490.0029   -35490411.2547           55.57s\n",
      "       130 151879040594.5521   -90898710.2704           52.95s\n",
      "       131 152161079410.7992   -50943281.7693           50.29s\n",
      "       132 150633420229.7563   -72680987.2904           47.65s\n",
      "       133 150402389898.5645   -95827199.3246           45.00s\n",
      "       134 150386240358.9295  -149373892.4503           42.36s\n",
      "       135 148539451529.0457   -91617678.3698           39.72s\n",
      "       136 148026125557.3489  -152594179.3559           37.06s\n",
      "       137 147738235160.8836  -135919098.5689           34.42s\n",
      "       138 148647870588.7383   -42769388.2694           31.79s\n",
      "       139 147459905211.2915   -87972719.0013           29.15s\n",
      "       140 146626350821.9782  -129472755.5533           26.50s\n",
      "       141 147085620126.3216   -64077514.8150           23.84s\n",
      "       142 146208776907.1013  -121293810.6334           21.19s\n",
      "       143 144487436730.0352  -191949556.5833           18.54s\n",
      "       144 144600889809.6130   -60961201.6574           15.89s\n",
      "       145 144482788745.7689   -63772650.7768           13.25s\n",
      "       146 144054688507.5265   -71021681.8064           10.60s\n",
      "       147 143599924574.2808  -133559031.0446            7.95s\n",
      "       148 142105528700.7687   -88210120.8272            5.30s\n",
      "       149 142868677783.3792   -34606784.2986            2.65s\n",
      "       150 142486172597.9045   -97403820.6438            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 292990667263.8625  6942866350.1891            6.40m\n",
      "         2 283296952247.4081  5547143136.5396            6.34m\n",
      "         3 273710559696.1052  4536984287.6308            6.34m\n",
      "         4 267673685647.2517  3641061259.5265            6.28m\n",
      "         5 262701382694.0127  2887672442.4427            6.24m\n",
      "         6 256734637360.6957  2228226836.6287            6.21m\n",
      "         7 252131130061.9483  1856605116.0106            6.17m\n",
      "         8 247410383116.4558  1268948498.3033            6.12m\n",
      "         9 245537745684.1456  1026849232.0908            6.10m\n",
      "        10 239495501117.4290   852304752.6073            6.06m\n",
      "        11 238683380629.6733   567943168.6226            6.03m\n",
      "        12 234670837147.1760   401334962.3770            5.99m\n",
      "        13 232194076141.4706   301869425.6930            5.96m\n",
      "        14 229617241939.4678   287433960.8733            5.92m\n",
      "        15 227656255476.7852   167595617.8306            5.89m\n",
      "        16 224231863597.2722    81334685.3431            5.85m\n",
      "        17 225141210225.3445    47207628.9406            5.81m\n",
      "        18 221743580401.7181    40631989.0533            5.77m\n",
      "        19 219166521160.9018   -91796265.6689            5.74m\n",
      "        20 219314464143.8438   -52940126.3454            5.70m\n",
      "        21 218471216999.0156   -86927493.1061            5.67m\n",
      "        22 214870968988.0090  -100778759.3407            5.63m\n",
      "        23 214291900584.1119   -79681367.2128            5.59m\n",
      "        24 212824885103.6158  -137523394.2188            5.55m\n",
      "        25 213398375475.3837   -71194603.0708            5.52m\n",
      "        26 210766375574.3582  -122240786.9590            5.48m\n",
      "        27 210306603748.1945  -181385724.3259            5.44m\n",
      "        28 208779364852.3047  -167606362.0292            5.39m\n",
      "        29 207522414596.7982  -107387139.3959            5.35m\n",
      "        30 207564817191.7288  -184458549.7845            5.31m\n",
      "        31 206167866448.4392   -84952141.7340            5.27m\n",
      "        32 203148938240.4119  -100896285.3176            5.23m\n",
      "        33 204820933669.9797  -157305248.5797            5.19m\n",
      "        34 203857708719.8335   -71781296.4466            5.15m\n",
      "        35 201690667487.9066   -80431080.2288            5.11m\n",
      "        36 202079117530.8925  -123935923.3844            5.06m\n",
      "        37 200696838440.0280   -86658791.9425            5.02m\n",
      "        38 200810052001.2746   -98442644.8989            4.98m\n",
      "        39 200834269356.1024  -154777167.1734            4.94m\n",
      "        40 198612903836.2607   -94774241.8242            4.90m\n",
      "        41 197975549368.2104  -125889374.5147            4.85m\n",
      "        42 197047393875.4562   -98725823.7746            4.81m\n",
      "        43 196531129303.6729   -86953723.4957            4.77m\n",
      "        44 195397186857.7133  -129708555.4815            4.73m\n",
      "        45 193542634992.1763  -189467403.3199            4.68m\n",
      "        46 193979091988.2500   -77588218.0588            4.63m\n",
      "        47 194933347642.7762   -69421699.8776            4.59m\n",
      "        48 192565897554.3959  -134339573.9473            4.55m\n",
      "        49 193481802004.5730   -88926965.4201            4.51m\n",
      "        50 192787199400.6533   -92179946.5425            4.46m\n",
      "        51 190144213293.1160  -184292080.8010            4.42m\n",
      "        52 191091848190.4106   -81392995.8770            4.37m\n",
      "        53 189690551329.7495   -92787598.5511            4.33m\n",
      "        54 189288545688.6031  -124399462.0955            4.28m\n",
      "        55 189374250442.8636   -94509345.1630            4.24m\n",
      "        56 187391601211.6277  -122842313.7144            4.20m\n",
      "        57 187624829398.3152   -98450736.7952            4.15m\n",
      "        58 187841806671.2685   -32027092.6354            4.10m\n",
      "        59 187200577641.1586  -168531136.0893            4.06m\n",
      "        60 186561323029.3751  -109487846.0082            4.01m\n",
      "        61 184784350824.0898   -79445761.0147            3.96m\n",
      "        62 183854312983.1363   -85121843.6917            3.92m\n",
      "        63 183742755924.4026   -60589361.7903            3.88m\n",
      "        64 183273708999.1455  -147558533.1009            3.83m\n",
      "        65 182131314926.7398  -193847354.1974            3.79m\n",
      "        66 183692404719.9412   -42521093.4086            3.75m\n",
      "        67 182338672122.2722   -69676854.0004            3.70m\n",
      "        68 181527884519.3308   -78211912.4691            3.66m\n",
      "        69 180856678724.2495  -102876466.8314            3.62m\n",
      "        70 181269212964.5513   -23605307.9409            3.57m\n",
      "        71 179198674668.4415   -76920768.0554            3.52m\n",
      "        72 180226977752.0776   -94373957.4174            3.48m\n",
      "        73 179543891713.8206   -75799664.9051            3.43m\n",
      "        74 179151720403.7071   -79084986.4313            3.39m\n",
      "        75 178525579017.2408   -56427504.4014            3.34m\n",
      "        76 178643771666.3620   -63965026.9485            3.30m\n",
      "        77 176911430729.0360  -168915882.2318            3.25m\n",
      "        78 177958774457.3485   -91050001.9462            3.21m\n",
      "        79 175628117192.2753  -146881050.2343            3.16m\n",
      "        80 176150001727.3940   -49461758.6074            3.12m\n",
      "        81 176418469893.3552   -48102702.7382            3.07m\n",
      "        82 175357616506.0087  -109587057.9544            3.03m\n",
      "        83 173923850647.7279  -128935450.4490            2.98m\n",
      "        84 173637241550.4605  -103531301.9745            2.94m\n",
      "        85 173601018558.5946   -52500505.9312            2.89m\n",
      "        86 171676285751.5086  -134467746.2597            2.85m\n",
      "        87 171467282604.6059   -87231827.4398            2.81m\n",
      "        88 171382261610.1411   -92015572.7333            2.76m\n",
      "        89 171499908009.1266   -66143399.7242            2.72m\n",
      "        90 169074837897.7039  -141507492.0686            2.67m\n",
      "        91 171135825430.4239   -51417752.7381            2.63m\n",
      "        92 168823899506.8830  -137411699.8409            2.58m\n",
      "        93 168977568915.6378   -55996568.1881            2.54m\n",
      "        94 169267168757.1222  -102893337.4590            2.49m\n",
      "        95 167788191786.2084   -93248379.5130            2.45m\n",
      "        96 166399379103.1572  -188132374.2152            2.40m\n",
      "        97 166792069029.9648   -96106307.6067            2.36m\n",
      "        98 165974671791.7267  -142166148.0221            2.31m\n",
      "        99 167338673528.2055   -59078684.2927            2.27m\n",
      "       100 165849439783.3126  -102328282.8146            2.23m\n",
      "       101 164335169124.2697  -117311006.3879            2.18m\n",
      "       102 163794190944.7650   -94002490.5740            2.14m\n",
      "       103 164777714341.9466   -81792756.7470            2.09m\n",
      "       104 161975362552.9661   -76955512.0667            2.05m\n",
      "       105 164097722779.0447   -38480446.9290            2.00m\n",
      "       106 161902384915.3935   -82779045.9820            1.96m\n",
      "       107 162068752602.7464   -80850983.9118            1.91m\n",
      "       108 161757594068.8208  -117914220.5496            1.87m\n",
      "       109 160617457032.7402   -95143696.2524            1.83m\n",
      "       110 160163109938.3258  -103767811.2749            1.78m\n",
      "       111 160062790625.7219   -67259670.5984            1.74m\n",
      "       112 159669723775.6161   -46869855.0209            1.69m\n",
      "       113 159769783872.2683   -57136908.7682            1.65m\n",
      "       114 159176940068.8136  -134613625.6884            1.60m\n",
      "       115 159321398672.3036   -49455986.2488            1.56m\n",
      "       116 158830140617.0841   -84165848.8539            1.51m\n",
      "       117 158229730139.6154  -110952198.0036            1.47m\n",
      "       118 157072291081.7865  -119205882.9700            1.42m\n",
      "       119 157553847148.2174   -44414157.3801            1.38m\n",
      "       120 156897054411.2235   -79708831.9570            1.33m\n",
      "       121 156023518175.4301   -46054955.8971            1.29m\n",
      "       122 155746995007.0729   -53687653.1346            1.25m\n",
      "       123 155522385671.4604  -111176177.6828            1.20m\n",
      "       124 154318482423.2849   -61268702.3807            1.16m\n",
      "       125 155397891647.5759  -150873229.7184            1.11m\n",
      "       126 153958683939.7581   -67911327.8188            1.07m\n",
      "       127 154755619614.9801   -61505082.0542            1.02m\n",
      "       128 153276040626.5117  -169415312.6390           58.73s\n",
      "       129 152469457850.5005   -77295356.2643           56.06s\n",
      "       130 152321424958.4307   -92582245.3726           53.40s\n",
      "       131 152449259659.5089   -32022092.0534           50.73s\n",
      "       132 151589210216.4879   -44938410.4962           48.06s\n",
      "       133 150936820418.2495   -64757260.5978           45.40s\n",
      "       134 150626098246.1265   -99312258.4930           42.74s\n",
      "       135 149706544515.1517   -85232898.9088           40.05s\n",
      "       136 149662441585.8094   -99753400.1356           37.38s\n",
      "       137 149711115104.4895   -47886244.8916           34.70s\n",
      "       138 148626770103.7099   -83471571.8513           32.03s\n",
      "       139 148619096180.1894   -68441800.3325           29.37s\n",
      "       140 147400695960.2496   -70652201.6507           26.70s\n",
      "       141 148452929260.1133   -44414651.5400           24.02s\n",
      "       142 147776546869.5973   -96714835.2543           21.35s\n",
      "       143 146545637587.9792  -127836554.1175           18.68s\n",
      "       144 145071545733.0316   -72261488.8259           16.02s\n",
      "       145 146039581317.9030   -89271141.3156           13.35s\n",
      "       146 145106970970.5800   -73515201.0330           10.68s\n",
      "       147 144204618688.9041   -90685593.7814            8.01s\n",
      "       148 143334309212.0806  -163288899.6927            5.34s\n",
      "       149 143383331953.8094  -101393628.1319            2.67s\n",
      "       150 143168136484.6523  -148893080.0680            0.00s\n"
     ]
    }
   ],
   "source": [
    "GBReg = GradientBoostingRegressor(n_estimators=150, \n",
    "                                  max_depth = 15,\n",
    "                                  random_state = 101,\n",
    "                                  subsample = 0.5,\n",
    "                                  verbose = 2)\n",
    "\n",
    "scores = cross_validate(GBReg, X, Y,return_train_score = True, cv=5, scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e33038b-1dc9-495c-ae98-f47e5542dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0.5347840569702195\n",
      "Test: 0.06940931031451256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_r2'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_r2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f777ec43-76bc-4d3b-9bb4-eb2fdc91b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: -67.42334660879557\n",
      "Test: -103.82664997040061\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_neg_mean_absolute_percentage_error'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_neg_mean_absolute_percentage_error'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81934c5-4397-4996-bc4e-864cdeaa7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBReg = GradientBoostingRegressor(n_estimators=150, \n",
    "                                  max_depth = 30,\n",
    "                                  random_state = 101,\n",
    "                                  subsample = 0.5,\n",
    "                                  verbose = 2)\n",
    "\n",
    "scores = cross_validate(GBReg, X, Y,return_train_score = True, cv=5, scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb04fc7-2824-4d72-90ca-e542b8066236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_r2'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_r2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d7f05-b41a-4761-a580-98d0df36b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entrenamiento: {np.mean(scores['train_neg_mean_absolute_percentage_error'])}\")\n",
    "print(f\"Test: {np.mean(scores['test_neg_mean_absolute_percentage_error'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddff10-cdbf-47a4-9bff-51212d3d112d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
